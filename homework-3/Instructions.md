# CSE 262 - Homework 3

**Due: 09/15 by EOD**

**This assignment is out of 100 points. All questions are weighted equally.**

## Instructions

1. Fork this repository into your CSE262 project namespace. [Instructions](https://docs.gitlab.com/ee/workflow/forking_workflow.html#creating-a-fork)
2. Clone your newly forked repository onto your development machine. [Instructions](https://docs.gitlab.com/ee/gitlab-basics/start-using-git.html#clone-a-repository) 
3. As you are writing code you should commit patches along the way. *i.e.* don't just submit all your code in one big commit when you're all done. Commit your progress as you work. You should have at least one commit per question.
4. When you've committed all of your work, there's nothing left to do to submit the assignment.

## Introduction

In this assignment you will write a lexer for the Asa language. We will define the language in the next assignment, but for now consider this program written in the language:

```
fn foo(y: Int) = Int
  let x = 1;
  let y = x + y;
  return x;
end
```

Over the course of several assignments spread throughout this semester, we are going to write a front-end compiler for this language, which will turn the above program into an intermediate representation that can be executed on a virtual machine.

## Question 1 - Lay out your project

We are going to use Rust to implement the lexer as a library. Lay out your project directory as follows:

```
root
  | src
  |  | bin 
  |  |  | main.rs
  |  | lib.rs
  |  | lexer.rs
  | tests
  |  | lexer.rs
  | Cargo.toml
  | README.md
```

The files are described below:
- `main.rs` - An executable for testing your lexer.
- `lib.rs` - Primary file for your library. Contains bookkeeping like imports and exports.
- `src/lexer.rs` - Lexer logic is implemented here.
- `tests/lexer.rs` - Tests for the lexer go here.
- `Cargo.toml` - Tells Cargo how to build the project. You can copy this from another project, or use `cargo new` to have Cargo generate one for you.
- `README.md` - This document.

## Question 2 - Stub in your structs

In `lexer.rs`, make a dummy lexer struct:

```rust
pub struct Lexer {

}

impl Lexer {

  pub fn new() -> Lexer {
    Lexer {

    }
  }

  pub fn lex(&mut self, source: String) {

  }
}
```

Export this struct from your library. In main.rs, add a blank `main()` function, and at the top of the file add the following lines:

```rust
extern crate lexer;
use lexer::Lexer;
```

Inside the main file, add the following lines:

```rust
let lexer = Lexer::new();
lexer.lex(String::from("let x = 1;"));
```

At this point you should almost be able to build your library with Cargo. You may have to add a couple additional crate imports to get it compiling. Follow the error messages indicated by Cargo, and make changes to the code as necessary.

## Question 3 - Write your first test

In `tests/lexer.rs`, set up the testing framework by importing your lexer crate (use the import statements from the previous question). Then write a test for the `Lexer::new()` function.

```rust
#[test]
fn question_3_lexer_new() {
  let lexer: Lexer = Lexer::new();
}
```

This test will call the `new()` function and makes sure that it returns a `Lexer` struct. Run your test by invoking

```
> cargo test
```

from the command line. You should see the following result:

```
running 1 test
test question_3_lexer_new ... ok

test result: ok. 1 passed; 0 failed; 0 ignored; 0 measured; 0 filtered out
```

## Question 4 - Read a file

Now we need to read a `*.asa` file from the hard drive into our program. There are examples on how to read a file [here](https://doc.rust-lang.org/stable/rust-by-example/std_misc/file.html). The [std::fs documentation](https://doc.rust-lang.org/std/fs/index.html) may come in handy.

In `src/lib.rs`, write a function that takes a path to a file, and returns the contents as a `String`.

```
fn read_file(path: std::Path) -> std::io::Result<String>
```

This function will take a [std::Path](https://doc.rust-lang.org/std/path/struct.Path.html) and use it to open a file. It will then read the contents line by line and concatenate them into a String, which is returned as a result.

Remember how [results](https://doc.rust-lang.org/std/result/index.html) work:

```rust
pub enum Result<T, E> {
  Ok(T),
  Err(E),
}
```

A result can either be `Ok()` of type T, or an `Err()` of type E. For us, T is a String and E is an [std::io::Error](https://doc.rust-lang.org/std/io/struct.Error.html) (don't forget to wrap it in an `Ok()`). If there is an error, it will be generated by the `open()` or `read_line()` functions. You can use the `?` syntax sugar to prematurely quit out of a function that receives and `Err()`. For example:

```rust
let mut file = File::open(path)?;
```

This will unwrap the `Ok(file)` into just the file handle if `File::open()` successfully opens the file. Otherwise (*e.g.* if the file doesn't exist) an `Err(error)` is returned immediately from the function.

You can't use [std::fs::read_to_string()](https://doc.rust-lang.org/std/fs/fn.read_to_string.html) or similar convenience functions to do this question. Although, you may want to study how this is implemented to help you (the source is accessible from the page linked above).

**Write two tests for this function in `tests.rs`**. Call them `question_4_read_file_ok()` and `question_4_read_file_err()`. They should check the case where the file exists and is read, and the failure case where the file does not exist. 

**This problem is worth maximum 50% without tests**

## Question 5 - Tokens

Now we will model the output of the lexer. The idea of the lexer is to categorize every byte of the input source code. We will call the byte along with its categorization a "Token".

In `lexer.rs`, write a struct called `Token`. It should have two fields, one called `type: TokenType`, and the other called `byte: u8`. Also, make an enum called `TokenType`. The categories will be as follows:

- Alpha - any alphabetical character, upper or lower case
- Digit - 0 to 9
- Whitespace - space, tab, newline, carriage return
- Grouping - parens, square brackets, angle brackets, curly braces
- Symbol - Anything other character.

## Question 6 - tokenize()

In `lexer.rs`, write a function called `tokenize`. It will take a byte, and return a Token. You will decide the `TokenType` of the byte using its ASCII value and a `match` statement.

You may find an [ASCII table](http://www.asciitable.com) and the match statement to help you with this task.

For example, for the byte `6A`, the ASCII character is `j`. Therefore the token would be

```rust
Token { byte: 0x6a, type: TokenType::Alpha }
```

In `tests/lexer.rs`, write at least 5 tests for `tokenize()` -- one for each TokenType. Name the tests:

- question_6_token_alpha()
- question_6_token_digit()
- question_6_token_whitespace()
- question_6_token_grouping()
- question_6_token_symbol()

**This problem is worth maximum 50% without tests**

## Question 7 - lex()

In `lexer.rs`, fill in the `lex()` function. This function should take in a `String` and return a `Vec<Token>`. It will operate by iterating over every byte of the input string, and calling `tokenize()` with it. 

***challenge:** see if you can implement this function in one line*

In `tests/lexer.rs`, write at least 5 tests for `lex()`. Name the tests:

- question_7_lex_1()
- question_7_lex_2()
- question_7_lex_3()
- question_7_lex_4()
- question_7_lex_5()

**This problem is worth maximum 50% without tests**

## Question 8 - Putting it all together

Now we are ready to test our lexer in `main.rs`. First, make it so the executable expects a string as the first command lin argument. You may find the [`std::env::args` docs](https://doc.rust-lang.org/1.39.0/std/env/fn.args.html) useful for this purpose. Personally, I like the [clap crate](https://crates.io/crates/clap) for parsing command line arguments.

The string argument will be interpreted as the path to a `*.asa` source code file. Open this file and read the contents into a string, and then lex it with the lexer, which will generate the vector of tokens.

## Question 9 - Writeup

For this final question, you will replace this document (you can just rename it, but replace it with a blank README.md file) with answers to the following questions:

1. Describe what a lexer is and what its place is in the compilation pipeline. 
2. Describe how you implemented the `lex()` and `tokenize()` functions.
3. Some character encodings use more than one byte per visible character (*e.g.* Unicode). How might you modify your lexer to handle multi-byte tokens? 